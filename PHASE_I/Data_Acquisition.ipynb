{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1: DATA ACQUISITION AND INGESTION\n",
    "## DATA ACQUISITION\n",
    "\n",
    "The purpose of this module is to  \n",
    "1. Download the Financial Statement Data Sets from the SEC website: https://www.sec.gov/dera/data/financial-statement-data-sets.html  \n",
    "2. Unzip the data and store locally.\n",
    "3. Merge the data in a dictionary of pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to scrape the SEC page\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# to download the zip file data\n",
    "import requests\n",
    "\n",
    "# to extract the zip files\n",
    "import zipfile\n",
    "\n",
    "# to interpret the zip file stream from requests\n",
    "# https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "import io\n",
    "\n",
    "# to work with paths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the SEC website\n",
    "page_url = 'https://www.sec.gov/dera/data/financial-statement-data-sets.html'\n",
    "r = requests.get(page_url)\n",
    "html_doc = r.text\n",
    "\n",
    "# To keep track of all year-quarters (eg 2014q3) discovered\n",
    "list_of_year_quarters = []\n",
    "\n",
    "# Find all hyperlinks pointing to .zip files\n",
    "soup = BeautifulSoup(html_doc)\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if '.zip' in a['href']:\n",
    "        # The hrefs are relative to the domain: add domain.\n",
    "        zip_file_url = 'https://www.sec.gov'+a['href']\n",
    "        year_quarter = zip_file_url[-10:-4]\n",
    "        list_of_year_quarters.append(year_quarter)\n",
    "        \n",
    "        # Avoid duplicate downloading\n",
    "        if not year_quarter in os.listdir('./SEC_Datasets'):\n",
    "            # Stream the file and print status update. Then, extract the zip file.\n",
    "            zip_data = requests.get(zip_file_url, stream=True)\n",
    "            with zipfile.ZipFile(io.BytesIO(zip_data.content), 'r') as zip_data_file:\n",
    "                zip_data_file.extractall('./SEC_Datasets/'+year_quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INGESTION\n",
    "\n",
    "The purpose of this module is to  \n",
    "1. Reads the raw .txt data files, acquired from SEC.gov, into Pandas DataFrames.\n",
    "2. Merges all quarterly files into a dictionary of DataFrames.\n",
    "3. Pickle this dictionary for future use.\n",
    "4. Generate and pickle a list of company names and state present in this dataset. This will form the input for the data gathering module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to work with data\n",
    "import pandas as pd\n",
    "\n",
    "# to write dictionary to disk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already processed\n"
     ]
    }
   ],
   "source": [
    "# Read files into Pandas\n",
    "# The desired object is a DICTIONARY of DATAFRAMES. Each DATAFRAME consists of all quarterly\n",
    "# files appended. eg. DataFrames['NUM'] yields a large dataframe with all NUM data across\n",
    "# all datasets downloaded.\n",
    "\n",
    "data_file_names = ['NUM','PRE','SUB']\n",
    "\n",
    "# to avoid duplicate effort during editing.\n",
    "if 'dict_of_dfs_num_pre_sub_tag.p' in os.listdir('.'):\n",
    "    with open('dict_of_dfs_num_pre_sub_tag.p', 'rb') as picklefile:\n",
    "        DataFrames = pickle.load(picklefile)\n",
    "else:    \n",
    "    DataFrames = {}\n",
    "\n",
    "# to avoid duplicate effort during editing        \n",
    "if not DataFrames:\n",
    "    # process all files\n",
    "    results = {}\n",
    "    for name in data_file_names:\n",
    "        results[name] = []\n",
    "        for qtr in list_of_year_quarters:\n",
    "            filepath = \"./SEC_Datasets/\" + qtr + \"/\" + name + '.txt'\n",
    "            df = pd.read_csv(filepath, \n",
    "                     sep='\\t', \n",
    "                     # This analyzes the entire file in 1 pass, improving accuracy of formating etc.\n",
    "                     low_memory=False, \n",
    "                     header=0,\n",
    "                     # Although the docs indicate utf-8 encoding, there exist non-compliant characters.\n",
    "                     # Stack Overflow recommended trying latin-1, which works.\n",
    "                     encoding='latin-1',\n",
    "                     # To avoid pandas guessing different dtypes in different files.\n",
    "                     dtype='str')\n",
    "            results[name].append(df)\n",
    "            print('appended '+qtr+' to '+name)\n",
    "        DataFrames[name] = pd.concat(results[name])\n",
    "        print('merged '+name)\n",
    "            \n",
    "else: print('Files already processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adsh</th>\n",
       "      <th>cik</th>\n",
       "      <th>name</th>\n",
       "      <th>sic</th>\n",
       "      <th>countryba</th>\n",
       "      <th>stprba</th>\n",
       "      <th>cityba</th>\n",
       "      <th>zipba</th>\n",
       "      <th>bas1</th>\n",
       "      <th>bas2</th>\n",
       "      <th>...</th>\n",
       "      <th>period</th>\n",
       "      <th>fy</th>\n",
       "      <th>fp</th>\n",
       "      <th>filed</th>\n",
       "      <th>accepted</th>\n",
       "      <th>prevrpt</th>\n",
       "      <th>detail</th>\n",
       "      <th>instance</th>\n",
       "      <th>nciks</th>\n",
       "      <th>aciks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000002178-18-000067</td>\n",
       "      <td>2178</td>\n",
       "      <td>ADAMS RESOURCES &amp; ENERGY, INC.</td>\n",
       "      <td>5172</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>77027</td>\n",
       "      <td>17 S. BRIAR HOLLOW LN.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20180930</td>\n",
       "      <td>2018</td>\n",
       "      <td>Q3</td>\n",
       "      <td>20181107</td>\n",
       "      <td>2018-11-07 16:28:00.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ae-20180930_htm.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000002488-18-000189</td>\n",
       "      <td>2488</td>\n",
       "      <td>ADVANCED MICRO DEVICES INC</td>\n",
       "      <td>3674</td>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>SANTA CLARA</td>\n",
       "      <td>95054</td>\n",
       "      <td>2485 AUGUSTINE DRIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20180930</td>\n",
       "      <td>2018</td>\n",
       "      <td>Q3</td>\n",
       "      <td>20181031</td>\n",
       "      <td>2018-10-31 16:15:00.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>amd-20180929.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000002969-18-000044</td>\n",
       "      <td>2969</td>\n",
       "      <td>AIR PRODUCTS &amp; CHEMICALS INC /DE/</td>\n",
       "      <td>2810</td>\n",
       "      <td>US</td>\n",
       "      <td>PA</td>\n",
       "      <td>ALLENTOWN</td>\n",
       "      <td>18195-1501</td>\n",
       "      <td>7201 HAMILTON BLVD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20180930</td>\n",
       "      <td>2018</td>\n",
       "      <td>FY</td>\n",
       "      <td>20181120</td>\n",
       "      <td>2018-11-20 14:48:00.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>apd-10xkx30sep2018_htm.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   adsh   cik                               name   sic  \\\n",
       "0  0000002178-18-000067  2178     ADAMS RESOURCES & ENERGY, INC.  5172   \n",
       "1  0000002488-18-000189  2488         ADVANCED MICRO DEVICES INC  3674   \n",
       "2  0000002969-18-000044  2969  AIR PRODUCTS & CHEMICALS INC /DE/  2810   \n",
       "\n",
       "  countryba stprba       cityba       zipba                    bas1 bas2  ...  \\\n",
       "0        US     TX      HOUSTON       77027  17 S. BRIAR HOLLOW LN.  NaN  ...   \n",
       "1        US     CA  SANTA CLARA       95054    2485 AUGUSTINE DRIVE  NaN  ...   \n",
       "2        US     PA    ALLENTOWN  18195-1501      7201 HAMILTON BLVD  NaN  ...   \n",
       "\n",
       "     period    fy  fp     filed               accepted prevrpt detail  \\\n",
       "0  20180930  2018  Q3  20181107  2018-11-07 16:28:00.0       0      1   \n",
       "1  20180930  2018  Q3  20181031  2018-10-31 16:15:00.0       0      1   \n",
       "2  20180930  2018  FY  20181120  2018-11-20 14:48:00.0       0      1   \n",
       "\n",
       "                     instance nciks aciks  \n",
       "0         ae-20180930_htm.xml     1   NaN  \n",
       "1            amd-20180929.xml     1   NaN  \n",
       "2  apd-10xkx30sep2018_htm.xml     1   NaN  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrames['SUB'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already pickled\n"
     ]
    }
   ],
   "source": [
    "if not 'dict_of_dfs_num_pre_sub_tag.p' in os.listdir('.'):\n",
    "    with open('dict_of_dfs_num_pre_sub_tag.p', 'wb') as picklefile:\n",
    "        pickle.dump(DataFrames, picklefile)\n",
    "else:\n",
    "    print('file already pickled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "The working directory now contains the following data files:\n",
    "1. A folder 'SEC_Datasets' containing the .txt datafiles acquired from the SEC website.\n",
    "2. A pickle file 'dict_of_dfs_num_pre_sub_tag.p' containing the DICTIONARY of 3 DATAFRAMES of SEC data.  \n",
    "This will form the input for PHASE II - financial analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
